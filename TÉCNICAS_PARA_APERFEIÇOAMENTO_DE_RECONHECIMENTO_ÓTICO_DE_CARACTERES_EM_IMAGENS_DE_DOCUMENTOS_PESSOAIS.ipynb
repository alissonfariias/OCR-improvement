{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXkhGtXJR6Gbx9oM5Y7PHT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alissonfariias/OCR-improvement/blob/main/T%C3%89CNICAS_PARA_APERFEI%C3%87OAMENTO_DE_RECONHECIMENTO_%C3%93TICO_DE_CARACTERES_EM_IMAGENS_DE_DOCUMENTOS_PESSOAIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÇ Google drive"
      ],
      "metadata": {
        "id": "4RNc8jjTQqaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bM_MhNTzQx8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Bibliotecas necess√°rias"
      ],
      "metadata": {
        "id": "YHc4HPWJbFiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract\n",
        "!sudo apt-get install tesseract-ocr\n",
        "!pip install opencv-python\n",
        "!apt-get install tesseract-ocr-por\n",
        "!pip install pillow\n",
        "!pip install unidecode"
      ],
      "metadata": {
        "id": "Fk9DlM5kbvJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Importando as bibliotecas necess√°rias"
      ],
      "metadata": {
        "id": "-flz9E_RenPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "import os\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "import shutil\n",
        "import zipfile\n",
        "import difflib\n",
        "import nltk\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "oep92D7AfU6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÇ Organizando diret√≥rios referentes aos documentos de RGs"
      ],
      "metadata": {
        "id": "0s9U1L18dzJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminho para o diret√≥rio do TCC no Google Drive\n",
        "tcc_directory = '/content/drive/My Drive/TCC/RG/'\n",
        "\n",
        "# Caminho para o diret√≥rio de imagens\n",
        "imagens_directory = os.path.join(tcc_directory, 'images_rg')\n",
        "\n",
        "# Caminho para o diret√≥rio de ground_truth\n",
        "ground_truth_directory = os.path.join(tcc_directory, 'ground_truth_rg')\n",
        "\n",
        "# Diret√≥rio de sa√≠da para imagens pr√©-processadas\n",
        "output_dir_preprocessed_images = '/content/preprocessed_images/'\n",
        "os.makedirs(output_dir_preprocessed_images, exist_ok=True)\n",
        "\n",
        "# Diret√≥rio de sa√≠da para arquivos OCR brutos\n",
        "output_dir_ocr_files_raw = '/content/ocr_files_raw/'\n",
        "os.makedirs(output_dir_ocr_files_raw, exist_ok=True)\n",
        "\n",
        "# Diret√≥rio de sa√≠da para arquivos OCR com processamento\n",
        "output_dir_ocr_files_processed = '/content/ocr_files_processed/'\n",
        "os.makedirs(output_dir_ocr_files_processed, exist_ok=True)\n",
        "\n",
        "# Listar arquivos de imagens no diret√≥rio\n",
        "image_files = [f for f in os.listdir(imagens_directory) if f.endswith('.jpg')]"
      ],
      "metadata": {
        "id": "DU_BD_83uv5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÇüìÑ Manipula√ß√£o e pr√©-processamento dos arquivos 'ground truth' dos RGs"
      ],
      "metadata": {
        "id": "fuo4AVjGhnKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando diret√≥rio onde os arquivos ground truth originais ser√£o modificados e armazenados\n",
        "transcription_gt = '/content/transcription_gt'\n",
        "os.makedirs(transcription_gt, exist_ok=True)\n",
        "\n",
        "# Listar arquivos de ground_truth no diret√≥rio\n",
        "ground_truth_files = [f for f in os.listdir(ground_truth_directory) if f.endswith('.txt')]\n",
        "\n",
        "# Percorrendo os arquivos ground_truth carregados\n",
        "for gt_filename in ground_truth_files:\n",
        "    gt_path = os.path.join(ground_truth_directory, gt_filename)\n",
        "\n",
        "    with open(gt_path, 'r', encoding='latin-1') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Removendo o nome da coluna (transcription)\n",
        "    lines = lines[1:]\n",
        "\n",
        "    # Extraindo a coluna \"transcription\"\n",
        "    transcription_lines = [unidecode(line.strip().split(',')[-1].strip()) for line in lines]\n",
        "\n",
        "    # Caminho para o novo arquivo\n",
        "    new_gt_path = os.path.join(transcription_gt, gt_filename)\n",
        "\n",
        "    # Escrever as linhas no novo arquivo\n",
        "    with open(new_gt_path, 'w', encoding='latin-1') as new_file:\n",
        "        new_file.writelines([line + '\\n' for line in transcription_lines])"
      ],
      "metadata": {
        "id": "qtcxlth8MlmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÑ Processamento e OCR de imagens dos RGs"
      ],
      "metadata": {
        "id": "pkJtF7FgfbsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o para modificar o DPI da imagem e retorna a imagem com novo DPI.\n",
        "def modify_dpi(image, dpi=500):\n",
        "    new_width = int(image.shape[1] * (dpi / 300))\n",
        "    new_height = int(image.shape[0] * (dpi / 300))\n",
        "\n",
        "    # Redimensionar a imagem\n",
        "    modified_image = cv2.resize(image, (new_width, new_height))\n",
        "\n",
        "    return modified_image"
      ],
      "metadata": {
        "id": "TQpsr5mH0PBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Processar as imagens\n",
        "for image_filename in image_files:\n",
        "    image_path = os.path.join(imagens_directory, image_filename)\n",
        "    image_name, image_extension = os.path.splitext(image_filename)\n",
        "\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Realizar OCR na imagem original\n",
        "    textNO = pytesseract.image_to_string(image, lang='por')\n",
        "\n",
        "    # Salvar os OCR's das imagens originais em um arquivo .txt\n",
        "    output_text_raw_path = os.path.join(output_dir_ocr_files_raw, f\"{image_name}.txt\")\n",
        "    with open(output_text_raw_path, \"w\") as text_file:\n",
        "        text_file.write(textNO)\n",
        "\n",
        "    # Modificar o DPI da imagem para 500\n",
        "    modified_image = modify_dpi(image, dpi=500)\n",
        "\n",
        "    # Aplicar suaviza√ß√£o para remover ru√≠do\n",
        "    blurred_image = cv2.GaussianBlur(modified_image, (5, 5), 0)\n",
        "\n",
        "    # Converter a imagem para escala de cinza\n",
        "    gray_image = cv2.cvtColor(blurred_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Salvar a imagem pr√©-processada\n",
        "    output_image_path = os.path.join(output_dir_preprocessed_images, f\"{image_name}.png\")\n",
        "    cv2.imwrite(output_image_path, gray_image)\n",
        "\n",
        "    # Realizar OCR na imagem com pr√©-processamento\n",
        "    text = pytesseract.image_to_string(gray_image, lang='por')\n",
        "\n",
        "    # Removendo acentua√ß√£o e deixando todas as letras MA√çUSCULAS\n",
        "    text_preprocess = unidecode(text).upper()\n",
        "\n",
        "    # Salvar os OCR's das imagens pr√©-processadas em um arquivo .txt\n",
        "    output_text_path = os.path.join(output_dir_ocr_files_processed, f\"{image_name}.txt\")\n",
        "    with open(output_text_path, \"w\") as text_file:\n",
        "        text_file.write(text_preprocess)"
      ],
      "metadata": {
        "id": "gOpSf4GZo6hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Avalia√ß√£o OCRs (RGs)"
      ],
      "metadata": {
        "id": "7II4huK8yfV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "# Caminhos para as pastas\n",
        "pasta_pre_processamento = \"/content/ocr_files_processed\"\n",
        "pasta_ocr_cru = \"/content/ocr_files_raw\"\n",
        "pasta_ground_truth = \"/content/transcription_gt\"\n",
        "\n",
        "# Fun√ß√£o que calcula o F1 Score\n",
        "def calcular_f1_score(precisao, revocacao):\n",
        "    return 2 * (precisao * revocacao) / (precisao + revocacao) if (precisao + revocacao) > 0 else 0\n",
        "\n",
        "# Listas para armazenar os valores F1-Score\n",
        "f1_scores_pre = []\n",
        "f1_scores_cru = []\n",
        "nomes_arquivos_pre = []\n",
        "\n",
        "for arquivo_ocr_pre, arquivo_ocr_cru, arquivo_gt in zip(os.listdir(pasta_pre_processamento), os.listdir(pasta_ocr_cru), os.listdir(pasta_ground_truth)):\n",
        "    caminho_ocr_pre = os.path.join(pasta_pre_processamento, arquivo_ocr_pre)\n",
        "    caminho_ocr_cru = os.path.join(pasta_ocr_cru, arquivo_ocr_cru)\n",
        "    caminho_gt = os.path.join(pasta_ground_truth, arquivo_gt)\n",
        "\n",
        "    with open(caminho_ocr_pre, 'r', encoding='utf-8') as file_ocr_pre, \\\n",
        "         open(caminho_ocr_cru, 'r', encoding='utf-8') as file_ocr_cru, \\\n",
        "         open(caminho_gt, 'r', encoding='utf-8') as file_gt:\n",
        "\n",
        "        texto_ocr_pre = file_ocr_pre.read()\n",
        "        texto_ocr_cru = file_ocr_cru.read()\n",
        "        texto_gt = file_gt.read()\n",
        "\n",
        "        # Calcular as m√©tricas para o OCR com pr√©-processamento\n",
        "        precisao_pre = difflib.SequenceMatcher(None, texto_ocr_pre, texto_gt).ratio()\n",
        "        revocacao_pre = difflib.SequenceMatcher(None, texto_gt, texto_ocr_pre).ratio()\n",
        "        f1_score_pre = calcular_f1_score(precisao_pre, revocacao_pre)\n",
        "        f1_scores_pre.append(f1_score_pre)\n",
        "\n",
        "        # Calcular as m√©tricas para o OCR cru\n",
        "        precisao_cru = difflib.SequenceMatcher(None, texto_ocr_cru, texto_gt).ratio()\n",
        "        revocacao_cru = difflib.SequenceMatcher(None, texto_gt, texto_ocr_cru).ratio()\n",
        "        f1_score_cru = calcular_f1_score(precisao_cru, revocacao_cru)\n",
        "        f1_scores_cru.append(f1_score_cru)\n",
        "\n",
        "# Calcular a m√©dia dos valores F1-Score\n",
        "media_f1_pre = sum(f1_scores_pre) / len(f1_scores_pre)\n",
        "media_f1_cru = sum(f1_scores_cru) / len(f1_scores_cru)\n",
        "\n",
        "print(f'M√©dia F1-Score OCR com pr√©-processamento e p√≥s-processamento: {media_f1_pre:.2f}')\n",
        "print(f'M√©dia F1-Score OCR original: {media_f1_cru:.2f}')"
      ],
      "metadata": {
        "id": "FdgrHIccqD-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bda722e-0656-40f6-d333-9292fd06bbaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M√©dia F1-Score OCR com pr√©-processamento e p√≥s-processamento: 0.53\n",
            "M√©dia F1-Score OCR original: 0.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÇüìÑ Manipula√ß√£o e pr√©-processamento dos arquivos 'ground truth' dos CPFs"
      ],
      "metadata": {
        "id": "ZPG6P64WlWF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminho para o diret√≥rio do TCC no Google Drive\n",
        "tcc_directory = '/content/drive/My Drive/TCC/CPF/'\n",
        "\n",
        "# Caminho para o diret√≥rio de ground_truth\n",
        "ground_truth_directory = os.path.join(tcc_directory, 'ground_truth_cpf')\n",
        "\n",
        "# Listar arquivos de ground_truth no diret√≥rio\n",
        "ground_truth_files = os.listdir(ground_truth_directory)\n",
        "\n",
        "# Express√£o regular para encontrar CPFs\n",
        "cpf_pattern = re.compile(r'\\b\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2}\\b')\n",
        "\n",
        "# Lista para armazenar todos os CPFs encontrados\n",
        "all_cpfs = []\n",
        "\n",
        "# Percorrendo os arquivos ground_truth carregados\n",
        "for gt_filename in ground_truth_files:\n",
        "    gt_path = os.path.join(ground_truth_directory, gt_filename)\n",
        "\n",
        "    with open(gt_path, 'r', encoding='latin-1') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Encontre todos os CPFs no texto usando a express√£o regular\n",
        "    cpfs = cpf_pattern.findall(text)\n",
        "\n",
        "    # Adicione os CPFs √† lista\n",
        "    all_cpfs.extend(cpfs)\n",
        "\n",
        "# Caminho para o novo arquivo √∫nico que conter√° todos os CPFs no ambiente do Google Colab\n",
        "output_file_path = '/content/all_cpfs_ground_truth.txt'\n",
        "\n",
        "# Escrever todos os CPFs no novo arquivo, alternando entre as linhas\n",
        "with open(output_file_path, 'w', encoding='latin-1') as output_file:\n",
        "    for cpf in all_cpfs:\n",
        "        output_file.write(cpf + '\\n')"
      ],
      "metadata": {
        "id": "Y23IKIkulVhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üÜî Processamento e OCR (com pr√©-processamento) das imagens dos CPFs\n"
      ],
      "metadata": {
        "id": "4LMB0J66GrV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o para modificar o DPI da imagem e retorna a imagem com novo DPI.\n",
        "def modify_dpi(image, dpi=500):\n",
        "    new_width = int(image.shape[1] * (dpi / 300))\n",
        "    new_height = int(image.shape[0] * (dpi / 300))\n",
        "\n",
        "    # Redimensionar a imagem\n",
        "    modified_image = cv2.resize(image, (new_width, new_height))\n",
        "\n",
        "    return modified_image\n",
        "\n",
        "def extrair_numero_cpf(texto):\n",
        "    cpf_regex = r'\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2}'\n",
        "    cpf_encontrado = re.search(cpf_regex, texto)\n",
        "\n",
        "    if cpf_encontrado:\n",
        "        return cpf_encontrado.group()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Pasta que cont√©m as imagens de CPFs\n",
        "images_cpf = '/content/drive/MyDrive/TCC/CPF/images_cpf'\n",
        "\n",
        "# Arquivo para salvar os n√∫meros de CPF\n",
        "output_file = 'ocr_all_cpfs_with_preprocess.txt'\n",
        "\n",
        "# Abra o arquivo para escrever\n",
        "with open(output_file, 'w') as file:\n",
        "    for image_cpf_name in os.listdir(images_cpf):\n",
        "        if image_cpf_name.endswith('.jpg'):\n",
        "            caminho_imagem = os.path.join(images_cpf, image_cpf_name)\n",
        "            imagem = cv2.imread(caminho_imagem)\n",
        "            imagem_cinza = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
        "            texto_reconhecido = pytesseract.image_to_string(imagem_cinza)\n",
        "            numero_cpf = extrair_numero_cpf(texto_reconhecido)\n",
        "\n",
        "\n",
        "\n",
        "            if numero_cpf is not None:\n",
        "                file.write(numero_cpf + '\\n')\n",
        "            else:\n",
        "                _, imagem_binaria = cv2.threshold(imagem, 60, 255, cv2.THRESH_BINARY)\n",
        "                imagem_cinza = cv2.cvtColor(imagem_binaria, cv2.COLOR_BGR2GRAY)\n",
        "                texto_reconhecido = pytesseract.image_to_string(imagem_cinza)\n",
        "                numero_cpf = extrair_numero_cpf(texto_reconhecido)\n",
        "                if numero_cpf is None:\n",
        "                    imagem = cv2.imread(caminho_imagem)\n",
        "                    modified_image = modify_dpi(imagem, dpi=700)\n",
        "                    imagem_cinza = cv2.cvtColor(modified_image, cv2.COLOR_BGR2GRAY)\n",
        "                    texto_reconhecido = pytesseract.image_to_string(imagem_cinza)\n",
        "                    numero_cpf = extrair_numero_cpf(texto_reconhecido)\n",
        "                    if numero_cpf is None:\n",
        "                        numero_cpf = '000.000.000-00'\n",
        "                        file.write(numero_cpf + '\\n')\n",
        "                    else:\n",
        "                        file.write(numero_cpf + '\\n')\n",
        "                else:\n",
        "                    file.write(numero_cpf + '\\n')"
      ],
      "metadata": {
        "id": "ICK5TgxfGtmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Avalia√ß√£o OCRs (CPFs)"
      ],
      "metadata": {
        "id": "Grrxgxeh69hN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminho para o arquivo de CPFs gerado pelo ground_truth\n",
        "ground_truth_file = '/content/all_cpfs_ground_truth.txt'\n",
        "\n",
        "# Caminho para o arquivo de CPFs gerado pelo OCR\n",
        "ocr_file = '/content/ocr_all_cpfs_with_preprocess.txt'\n",
        "\n",
        "# Ler os CPFs do arquivo do ground_truth\n",
        "with open(ground_truth_file, 'r', encoding='latin-1') as file:\n",
        "    ground_truth_cpfs = set(file.read().splitlines())\n",
        "\n",
        "# Ler os CPFs do arquivo do OCR\n",
        "with open(ocr_file, 'r') as file:\n",
        "    ocr_cpfs = set(file.read().splitlines())\n",
        "\n",
        "# Calcular a interse√ß√£o entre os CPFs do OCR e do ground_truth\n",
        "intersection = ground_truth_cpfs.intersection(ocr_cpfs)\n",
        "\n",
        "# Calcular a precis√£o (accuracy)\n",
        "accuracy = len(intersection) / len(ground_truth_cpfs) * 100\n",
        "\n",
        "print(f\"Precis√£o (Accuracy): {accuracy:.2f}%\")\n",
        "\n",
        "# Calcular os CPFs que est√£o no ground_truth, mas n√£o est√£o no OCR\n",
        "missing_cpfs = ground_truth_cpfs.difference(ocr_cpfs)\n",
        "\n",
        "# Calcular a taxa de erro (error rate)\n",
        "error_rate = len(missing_cpfs) / len(ground_truth_cpfs) * 100\n",
        "\n",
        "print(f\"Taxa de Erro (Error Rate): {error_rate:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDwN5WTO69I-",
        "outputId": "acf9062f-67ef-4faa-c222-8048afbcbd95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precis√£o (Accuracy): 73.48%\n",
            "Taxa de Erro (Error Rate): 26.52%\n"
          ]
        }
      ]
    }
  ]
}