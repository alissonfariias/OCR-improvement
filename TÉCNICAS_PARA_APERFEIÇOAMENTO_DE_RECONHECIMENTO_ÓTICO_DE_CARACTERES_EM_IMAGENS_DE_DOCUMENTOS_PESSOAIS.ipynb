{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXkhGtXJR6Gbx9oM5Y7PHT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alissonfariias/OCR-improvement/blob/main/T%C3%89CNICAS_PARA_APERFEI%C3%87OAMENTO_DE_RECONHECIMENTO_%C3%93TICO_DE_CARACTERES_EM_IMAGENS_DE_DOCUMENTOS_PESSOAIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📂 Google drive"
      ],
      "metadata": {
        "id": "4RNc8jjTQqaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bM_MhNTzQx8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📦 Bibliotecas necessárias"
      ],
      "metadata": {
        "id": "YHc4HPWJbFiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract\n",
        "!sudo apt-get install tesseract-ocr\n",
        "!pip install opencv-python\n",
        "!apt-get install tesseract-ocr-por\n",
        "!pip install pillow\n",
        "!pip install unidecode"
      ],
      "metadata": {
        "id": "Fk9DlM5kbvJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📚 Importando as bibliotecas necessárias"
      ],
      "metadata": {
        "id": "-flz9E_RenPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "import os\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "import shutil\n",
        "import zipfile\n",
        "import difflib\n",
        "import nltk\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "oep92D7AfU6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📂 Organizando diretórios referentes aos documentos de RGs"
      ],
      "metadata": {
        "id": "0s9U1L18dzJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminho para o diretório do TCC no Google Drive\n",
        "tcc_directory = '/content/drive/My Drive/TCC/RG/'\n",
        "\n",
        "# Caminho para o diretório de imagens\n",
        "imagens_directory = os.path.join(tcc_directory, 'images_rg')\n",
        "\n",
        "# Caminho para o diretório de ground_truth\n",
        "ground_truth_directory = os.path.join(tcc_directory, 'ground_truth_rg')\n",
        "\n",
        "# Diretório de saída para imagens pré-processadas\n",
        "output_dir_preprocessed_images = '/content/preprocessed_images/'\n",
        "os.makedirs(output_dir_preprocessed_images, exist_ok=True)\n",
        "\n",
        "# Diretório de saída para arquivos OCR brutos\n",
        "output_dir_ocr_files_raw = '/content/ocr_files_raw/'\n",
        "os.makedirs(output_dir_ocr_files_raw, exist_ok=True)\n",
        "\n",
        "# Diretório de saída para arquivos OCR com processamento\n",
        "output_dir_ocr_files_processed = '/content/ocr_files_processed/'\n",
        "os.makedirs(output_dir_ocr_files_processed, exist_ok=True)\n",
        "\n",
        "# Listar arquivos de imagens no diretório\n",
        "image_files = [f for f in os.listdir(imagens_directory) if f.endswith('.jpg')]"
      ],
      "metadata": {
        "id": "DU_BD_83uv5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📂📄 Manipulação e pré-processamento dos arquivos 'ground truth' dos RGs"
      ],
      "metadata": {
        "id": "fuo4AVjGhnKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando diretório onde os arquivos ground truth originais serão modificados e armazenados\n",
        "transcription_gt = '/content/transcription_gt'\n",
        "os.makedirs(transcription_gt, exist_ok=True)\n",
        "\n",
        "# Listar arquivos de ground_truth no diretório\n",
        "ground_truth_files = [f for f in os.listdir(ground_truth_directory) if f.endswith('.txt')]\n",
        "\n",
        "# Percorrendo os arquivos ground_truth carregados\n",
        "for gt_filename in ground_truth_files:\n",
        "    gt_path = os.path.join(ground_truth_directory, gt_filename)\n",
        "\n",
        "    with open(gt_path, 'r', encoding='latin-1') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Removendo o nome da coluna (transcription)\n",
        "    lines = lines[1:]\n",
        "\n",
        "    # Extraindo a coluna \"transcription\"\n",
        "    transcription_lines = [unidecode(line.strip().split(',')[-1].strip()) for line in lines]\n",
        "\n",
        "    # Caminho para o novo arquivo\n",
        "    new_gt_path = os.path.join(transcription_gt, gt_filename)\n",
        "\n",
        "    # Escrever as linhas no novo arquivo\n",
        "    with open(new_gt_path, 'w', encoding='latin-1') as new_file:\n",
        "        new_file.writelines([line + '\\n' for line in transcription_lines])"
      ],
      "metadata": {
        "id": "qtcxlth8MlmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📄 Processamento e OCR de imagens dos RGs"
      ],
      "metadata": {
        "id": "pkJtF7FgfbsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para modificar o DPI da imagem e retorna a imagem com novo DPI.\n",
        "def modify_dpi(image, dpi=500):\n",
        "    new_width = int(image.shape[1] * (dpi / 300))\n",
        "    new_height = int(image.shape[0] * (dpi / 300))\n",
        "\n",
        "    # Redimensionar a imagem\n",
        "    modified_image = cv2.resize(image, (new_width, new_height))\n",
        "\n",
        "    return modified_image"
      ],
      "metadata": {
        "id": "TQpsr5mH0PBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Processar as imagens\n",
        "for image_filename in image_files:\n",
        "    image_path = os.path.join(imagens_directory, image_filename)\n",
        "    image_name, image_extension = os.path.splitext(image_filename)\n",
        "\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Realizar OCR na imagem original\n",
        "    textNO = pytesseract.image_to_string(image, lang='por')\n",
        "\n",
        "    # Salvar os OCR's das imagens originais em um arquivo .txt\n",
        "    output_text_raw_path = os.path.join(output_dir_ocr_files_raw, f\"{image_name}.txt\")\n",
        "    with open(output_text_raw_path, \"w\") as text_file:\n",
        "        text_file.write(textNO)\n",
        "\n",
        "    # Modificar o DPI da imagem para 500\n",
        "    modified_image = modify_dpi(image, dpi=500)\n",
        "\n",
        "    # Aplicar suavização para remover ruído\n",
        "    blurred_image = cv2.GaussianBlur(modified_image, (5, 5), 0)\n",
        "\n",
        "    # Converter a imagem para escala de cinza\n",
        "    gray_image = cv2.cvtColor(blurred_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Salvar a imagem pré-processada\n",
        "    output_image_path = os.path.join(output_dir_preprocessed_images, f\"{image_name}.png\")\n",
        "    cv2.imwrite(output_image_path, gray_image)\n",
        "\n",
        "    # Realizar OCR na imagem com pré-processamento\n",
        "    text = pytesseract.image_to_string(gray_image, lang='por')\n",
        "\n",
        "    # Removendo acentuação e deixando todas as letras MAÍUSCULAS\n",
        "    text_preprocess = unidecode(text).upper()\n",
        "\n",
        "    # Salvar os OCR's das imagens pré-processadas em um arquivo .txt\n",
        "    output_text_path = os.path.join(output_dir_ocr_files_processed, f\"{image_name}.txt\")\n",
        "    with open(output_text_path, \"w\") as text_file:\n",
        "        text_file.write(text_preprocess)"
      ],
      "metadata": {
        "id": "gOpSf4GZo6hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 Avaliação OCRs (RGs)"
      ],
      "metadata": {
        "id": "7II4huK8yfV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "# Caminhos para as pastas\n",
        "pasta_pre_processamento = \"/content/ocr_files_processed\"\n",
        "pasta_ocr_cru = \"/content/ocr_files_raw\"\n",
        "pasta_ground_truth = \"/content/transcription_gt\"\n",
        "\n",
        "# Função que calcula o F1 Score\n",
        "def calcular_f1_score(precisao, revocacao):\n",
        "    return 2 * (precisao * revocacao) / (precisao + revocacao) if (precisao + revocacao) > 0 else 0\n",
        "\n",
        "# Listas para armazenar os valores F1-Score\n",
        "f1_scores_pre = []\n",
        "f1_scores_cru = []\n",
        "nomes_arquivos_pre = []\n",
        "\n",
        "for arquivo_ocr_pre, arquivo_ocr_cru, arquivo_gt in zip(os.listdir(pasta_pre_processamento), os.listdir(pasta_ocr_cru), os.listdir(pasta_ground_truth)):\n",
        "    caminho_ocr_pre = os.path.join(pasta_pre_processamento, arquivo_ocr_pre)\n",
        "    caminho_ocr_cru = os.path.join(pasta_ocr_cru, arquivo_ocr_cru)\n",
        "    caminho_gt = os.path.join(pasta_ground_truth, arquivo_gt)\n",
        "\n",
        "    with open(caminho_ocr_pre, 'r', encoding='utf-8') as file_ocr_pre, \\\n",
        "         open(caminho_ocr_cru, 'r', encoding='utf-8') as file_ocr_cru, \\\n",
        "         open(caminho_gt, 'r', encoding='utf-8') as file_gt:\n",
        "\n",
        "        texto_ocr_pre = file_ocr_pre.read()\n",
        "        texto_ocr_cru = file_ocr_cru.read()\n",
        "        texto_gt = file_gt.read()\n",
        "\n",
        "        # Calcular as métricas para o OCR com pré-processamento\n",
        "        precisao_pre = difflib.SequenceMatcher(None, texto_ocr_pre, texto_gt).ratio()\n",
        "        revocacao_pre = difflib.SequenceMatcher(None, texto_gt, texto_ocr_pre).ratio()\n",
        "        f1_score_pre = calcular_f1_score(precisao_pre, revocacao_pre)\n",
        "        f1_scores_pre.append(f1_score_pre)\n",
        "\n",
        "        # Calcular as métricas para o OCR cru\n",
        "        precisao_cru = difflib.SequenceMatcher(None, texto_ocr_cru, texto_gt).ratio()\n",
        "        revocacao_cru = difflib.SequenceMatcher(None, texto_gt, texto_ocr_cru).ratio()\n",
        "        f1_score_cru = calcular_f1_score(precisao_cru, revocacao_cru)\n",
        "        f1_scores_cru.append(f1_score_cru)\n",
        "\n",
        "# Calcular a média dos valores F1-Score\n",
        "media_f1_pre = sum(f1_scores_pre) / len(f1_scores_pre)\n",
        "media_f1_cru = sum(f1_scores_cru) / len(f1_scores_cru)\n",
        "\n",
        "print(f'Média F1-Score OCR com pré-processamento e pós-processamento: {media_f1_pre:.2f}')\n",
        "print(f'Média F1-Score OCR original: {media_f1_cru:.2f}')"
      ],
      "metadata": {
        "id": "FdgrHIccqD-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bda722e-0656-40f6-d333-9292fd06bbaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Média F1-Score OCR com pré-processamento e pós-processamento: 0.53\n",
            "Média F1-Score OCR original: 0.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📂📄 Manipulação e pré-processamento dos arquivos 'ground truth' dos CPFs"
      ],
      "metadata": {
        "id": "ZPG6P64WlWF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminho para o diretório do TCC no Google Drive\n",
        "tcc_directory = '/content/drive/My Drive/TCC/CPF/'\n",
        "\n",
        "# Caminho para o diretório de ground_truth\n",
        "ground_truth_directory = os.path.join(tcc_directory, 'ground_truth_cpf')\n",
        "\n",
        "# Listar arquivos de ground_truth no diretório\n",
        "ground_truth_files = os.listdir(ground_truth_directory)\n",
        "\n",
        "# Expressão regular para encontrar CPFs\n",
        "cpf_pattern = re.compile(r'\\b\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2}\\b')\n",
        "\n",
        "# Lista para armazenar todos os CPFs encontrados\n",
        "all_cpfs = []\n",
        "\n",
        "# Percorrendo os arquivos ground_truth carregados\n",
        "for gt_filename in ground_truth_files:\n",
        "    gt_path = os.path.join(ground_truth_directory, gt_filename)\n",
        "\n",
        "    with open(gt_path, 'r', encoding='latin-1') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Encontre todos os CPFs no texto usando a expressão regular\n",
        "    cpfs = cpf_pattern.findall(text)\n",
        "\n",
        "    # Adicione os CPFs à lista\n",
        "    all_cpfs.extend(cpfs)\n",
        "\n",
        "# Caminho para o novo arquivo único que conterá todos os CPFs no ambiente do Google Colab\n",
        "output_file_path = '/content/all_cpfs_ground_truth.txt'\n",
        "\n",
        "# Escrever todos os CPFs no novo arquivo, alternando entre as linhas\n",
        "with open(output_file_path, 'w', encoding='latin-1') as output_file:\n",
        "    for cpf in all_cpfs:\n",
        "        output_file.write(cpf + '\\n')"
      ],
      "metadata": {
        "id": "Y23IKIkulVhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🆔 Processamento e OCR (com pré-processamento) das imagens dos CPFs\n"
      ],
      "metadata": {
        "id": "4LMB0J66GrV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para modificar o DPI da imagem e retorna a imagem com novo DPI.\n",
        "def modify_dpi(image, dpi=500):\n",
        "    new_width = int(image.shape[1] * (dpi / 300))\n",
        "    new_height = int(image.shape[0] * (dpi / 300))\n",
        "\n",
        "    # Redimensionar a imagem\n",
        "    modified_image = cv2.resize(image, (new_width, new_height))\n",
        "\n",
        "    return modified_image\n",
        "\n",
        "def extrair_numero_cpf(texto):\n",
        "    cpf_regex = r'\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2}'\n",
        "    cpf_encontrado = re.search(cpf_regex, texto)\n",
        "\n",
        "    if cpf_encontrado:\n",
        "        return cpf_encontrado.group()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Pasta que contém as imagens de CPFs\n",
        "images_cpf = '/content/drive/MyDrive/TCC/CPF/images_cpf'\n",
        "\n",
        "# Arquivo para salvar os números de CPF\n",
        "output_file = 'ocr_all_cpfs_with_preprocess.txt'\n",
        "\n",
        "# Abra o arquivo para escrever\n",
        "with open(output_file, 'w') as file:\n",
        "    for image_cpf_name in os.listdir(images_cpf):\n",
        "        if image_cpf_name.endswith('.jpg'):\n",
        "            caminho_imagem = os.path.join(images_cpf, image_cpf_name)\n",
        "            imagem = cv2.imread(caminho_imagem)\n",
        "            imagem_cinza = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
        "            texto_reconhecido = pytesseract.image_to_string(imagem_cinza)\n",
        "            numero_cpf = extrair_numero_cpf(texto_reconhecido)\n",
        "\n",
        "\n",
        "\n",
        "            if numero_cpf is not None:\n",
        "                file.write(numero_cpf + '\\n')\n",
        "            else:\n",
        "                _, imagem_binaria = cv2.threshold(imagem, 60, 255, cv2.THRESH_BINARY)\n",
        "                imagem_cinza = cv2.cvtColor(imagem_binaria, cv2.COLOR_BGR2GRAY)\n",
        "                texto_reconhecido = pytesseract.image_to_string(imagem_cinza)\n",
        "                numero_cpf = extrair_numero_cpf(texto_reconhecido)\n",
        "                if numero_cpf is None:\n",
        "                    imagem = cv2.imread(caminho_imagem)\n",
        "                    modified_image = modify_dpi(imagem, dpi=700)\n",
        "                    imagem_cinza = cv2.cvtColor(modified_image, cv2.COLOR_BGR2GRAY)\n",
        "                    texto_reconhecido = pytesseract.image_to_string(imagem_cinza)\n",
        "                    numero_cpf = extrair_numero_cpf(texto_reconhecido)\n",
        "                    if numero_cpf is None:\n",
        "                        numero_cpf = '000.000.000-00'\n",
        "                        file.write(numero_cpf + '\\n')\n",
        "                    else:\n",
        "                        file.write(numero_cpf + '\\n')\n",
        "                else:\n",
        "                    file.write(numero_cpf + '\\n')"
      ],
      "metadata": {
        "id": "ICK5TgxfGtmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 Avaliação OCRs (CPFs)"
      ],
      "metadata": {
        "id": "Grrxgxeh69hN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminho para o arquivo de CPFs gerado pelo ground_truth\n",
        "ground_truth_file = '/content/all_cpfs_ground_truth.txt'\n",
        "\n",
        "# Caminho para o arquivo de CPFs gerado pelo OCR\n",
        "ocr_file = '/content/ocr_all_cpfs_with_preprocess.txt'\n",
        "\n",
        "# Ler os CPFs do arquivo do ground_truth\n",
        "with open(ground_truth_file, 'r', encoding='latin-1') as file:\n",
        "    ground_truth_cpfs = set(file.read().splitlines())\n",
        "\n",
        "# Ler os CPFs do arquivo do OCR\n",
        "with open(ocr_file, 'r') as file:\n",
        "    ocr_cpfs = set(file.read().splitlines())\n",
        "\n",
        "# Calcular a interseção entre os CPFs do OCR e do ground_truth\n",
        "intersection = ground_truth_cpfs.intersection(ocr_cpfs)\n",
        "\n",
        "# Calcular a precisão (accuracy)\n",
        "accuracy = len(intersection) / len(ground_truth_cpfs) * 100\n",
        "\n",
        "print(f\"Precisão (Accuracy): {accuracy:.2f}%\")\n",
        "\n",
        "# Calcular os CPFs que estão no ground_truth, mas não estão no OCR\n",
        "missing_cpfs = ground_truth_cpfs.difference(ocr_cpfs)\n",
        "\n",
        "# Calcular a taxa de erro (error rate)\n",
        "error_rate = len(missing_cpfs) / len(ground_truth_cpfs) * 100\n",
        "\n",
        "print(f\"Taxa de Erro (Error Rate): {error_rate:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDwN5WTO69I-",
        "outputId": "acf9062f-67ef-4faa-c222-8048afbcbd95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisão (Accuracy): 73.48%\n",
            "Taxa de Erro (Error Rate): 26.52%\n"
          ]
        }
      ]
    }
  ]
}